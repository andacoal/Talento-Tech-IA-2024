{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Actividad: proceso de decisión de Markov\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Por: Ángela Córdoba\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "kZhd0GjYT2S2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Descripción MDP"
      ],
      "metadata": {
        "id": "GTcPiG2CV798"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hB6nIM55TwuI",
        "outputId": "4ec1e950-fff5-46f6-d12c-1633b0976a15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estado actual: A\n",
            "Acción tomada: Arriba\n",
            "Nuevo estado B\n",
            "Recompensa: 9\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Definicion de estados, acciones y recompensas\n",
        "estados = ['A', 'B', 'C']\n",
        "acciones = ['Arriba', 'Abajo']\n",
        "recompensas = np.random.randint(9, 10, size=(len(estados),len(acciones)))\n",
        "\n",
        "# Función de transición aleatoria\n",
        "def transicion_aleatoria():\n",
        "  return np.random.choice (estados)\n",
        "\n",
        "# Generación de datos\n",
        "estado_actual = np.random.choice(estados)\n",
        "accion = np.random.choice(acciones)\n",
        "nuevo_estado = transicion_aleatoria()\n",
        "recompensa = recompensas[estados.index(estado_actual), acciones.index(accion)]\n",
        "\n",
        "print(\"Estado actual:\", estado_actual)\n",
        "print(\"Acción tomada:\", accion)\n",
        "print(\"Nuevo estado\", nuevo_estado)\n",
        "print(\"Recompensa:\", recompensa)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Conceptos MDP"
      ],
      "metadata": {
        "id": "2bLuKzdwWMIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Definicion de estados, acciones y recompensas\n",
        "estados = ['A', 'B', 'C']\n",
        "acciones = ['Arriba', 'Abajo']\n",
        "recompensas = np.random.randint(9, 10, size=(len(estados),len(acciones)))\n",
        "\n",
        "# Función de transición aleatoria\n",
        "def transicion_aleatoria():\n",
        "  return np.random.choice (estados)\n",
        "\n",
        "# Generación de datos\n",
        "estado_actual = np.random.choice(estados)\n",
        "accion = np.random.choice(acciones)\n",
        "nuevo_estado = transicion_aleatoria()\n",
        "recompensa = recompensas[estados.index(estado_actual), acciones.index(accion)]\n",
        "\n",
        "print(\"Estado actual:\", estado_actual)\n",
        "print(\"Acción tomada:\", accion)\n",
        "print(\"Nuevo estado\", nuevo_estado)\n",
        "print(\"Recompensa:\", recompensa)\n",
        "\n",
        "class MDP:\n",
        "    def __init__(self, estados, acciones, recompensas, transiciones):\n",
        "        self.estados = estados\n",
        "        self.acciones = acciones\n",
        "        self.recompensas = recompensas\n",
        "        self.transiciones = transiciones\n",
        "\n",
        "# Define transiciones here before using it\n",
        "# Example:\n",
        "transiciones = {\n",
        "    'A': {'Arriba': {'A': 0.3, 'B': 0.6, 'C': 0.1}, 'Abajo': {'A': 0.8, 'B': 0.1, 'C': 0.1}},\n",
        "    'B': {'Arriba': {'A': 0.2, 'B': 0.7, 'C': 0.1}, 'Abajo': {'A': 0.1, 'B': 0.8, 'C': 0.1}},\n",
        "    'C': {'Arriba': {'A': 0.1, 'B': 0.2, 'C': 0.7}, 'Abajo': {'A': 0.1, 'B': 0.1, 'C': 0.8}}\n",
        "}\n",
        "\n",
        "def calcular_valor_estado(mdp, gamma=0.9, theta=0.01):\n",
        "    # Inicializar valores de los estados\n",
        "    valores = {estado: 0 for estado in mdp.estados}\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for estado in mdp.estados:\n",
        "            valor_previo = valores[estado]\n",
        "            # Calcular el nuevo valor del estado\n",
        "            valores[estado] = max(\n",
        "                sum(\n",
        "                    mdp.transiciones[estado][accion][nuevo_estado] *\n",
        "                    (mdp.recompensas[mdp.estados.index(estado)][mdp.acciones.index(accion)] + gamma * valores[nuevo_estado])\n",
        "                    for nuevo_estado in mdp.estados\n",
        "                )\n",
        "                for accion in mdp.acciones\n",
        "            )\n",
        "            # Actualizar delta\n",
        "            delta = max(delta, abs(valor_previo - valores[estado]))\n",
        "        # Salir del bucle si delta es menor que theta\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return valores\n",
        "\n",
        "# Ejemplo de uso\n",
        "mdp = MDP(estados, acciones, recompensas, transiciones) # Now transiciones is defined\n",
        "valores_estados = calcular_valor_estado(mdp)\n",
        "print(\"Valores de los estados:\", valores_estados)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IngkReyKYKmD",
        "outputId": "f0b3deb5-e4b0-4378-c7af-9fb3f8b4a06b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estado actual: C\n",
            "Acción tomada: Abajo\n",
            "Nuevo estado C\n",
            "Recompensa: 9\n",
            "Valores de los estados: {'A': 89.93051459309547, 'B': 89.93242770766095, 'C': 89.93503796782912}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Propiedades de Markov"
      ],
      "metadata": {
        "id": "ZauVEE3lYZsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def verificar_propiedad_markov(mdp):\n",
        "    for estado in mdp.estados:\n",
        "        for accion in mdp.acciones:\n",
        "            suma_probabilidades = sum(mdp.transiciones[estado][accion].values())\n",
        "            if not np.isclose(suma_probabilidades, 1):\n",
        "                return False\n",
        "    return True\n",
        "\n",
        "# Ejemplo de uso\n",
        "print(\"Cumple con la propiedad de Markov:\", verificar_propiedad_markov(mdp))"
      ],
      "metadata": {
        "id": "6w_ikWu8VUWD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13e9618b-2e91-483f-aea3-b707f1107546"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cumple con la propiedad de Markov: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Propiedad de recompensa"
      ],
      "metadata": {
        "id": "0mMDvgK4Y2xG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calcular_recompensa_promedio(mdp):\n",
        "    recompensa_total = 0\n",
        "    total_transiciones = 0  # Contador para el número total de transiciones\n",
        "\n",
        "    for estado in mdp.estados:\n",
        "        for accion in mdp.acciones:\n",
        "            for nuevo_estado in mdp.estados:\n",
        "                recompensa_total += mdp.recompensas[mdp.estados.index(estado)][mdp.acciones.index(accion)]\n",
        "                total_transiciones += 1\n",
        "\n",
        "    return recompensa_total / total_transiciones\n",
        "\n",
        "# Ejemplo de uso\n",
        "print(\"Recompensa promedio por acción:\", calcular_recompensa_promedio(mdp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrV_BLO2Yzsm",
        "outputId": "7e10f906-6cc3-408e-c484-96ef3a5471eb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recompensa promedio por acción: 9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oVADIiTHZQBe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}