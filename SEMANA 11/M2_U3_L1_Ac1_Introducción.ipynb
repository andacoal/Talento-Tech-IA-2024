{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Actividad\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Por: Ángela Córdoba\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "kZhd0GjYT2S2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hB6nIM55TwuI",
        "outputId": "960d7705-2816-4b92-8e88-57ee0b428c8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acción seleccionada por el agente: derecha\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "class AgenteRL:\n",
        "  def __init__(self, acciones):\n",
        "    self.acciones = acciones\n",
        "\n",
        "  def seleccionar_accion(self, estado):\n",
        "    return random.choice(self.acciones)\n",
        "\n",
        "# Uso del agente RL\n",
        "acciones_posibles = [\"arriba\", \"abajo\", \"izquierda\", \"derecha\"]\n",
        "agente_rl = AgenteRL(acciones_posibles)\n",
        "estado_actual = [0,0] # Estado inicial del entorno\n",
        "accion_seleccionada = agente_rl.seleccionar_accion(estado_actual)\n",
        "print(\"Acción seleccionada por el agente:\", accion_seleccionada)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EntornoRL:\n",
        "    def __init__(self, estados):\n",
        "        self.estados = estados\n",
        "\n",
        "    def tomar_accion(self,accion):\n",
        "      # Simulación de la transición de estado\n",
        "      nuevo_estado = random.choice(self.estados)\n",
        "      recompensa = random.randint(-10,10)\n",
        "      return nuevo_estado, recompensa\n",
        "\n",
        "# Uso del entorno RL\n",
        "estados_posibles = ['A', 'B', 'C', 'D']\n",
        "entorno_rl = EntornoRL(estados_posibles)\n",
        "accion = 'izquierda' # Acción seleccionada por el agente\n",
        "nuevo_estado, recompensa = entorno_rl.tomar_accion(accion)\n",
        "print(\"Nuevo estado:\", nuevo_estado)\n",
        "print(\"Recompensa recibida:\", recompensa)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tw4SAgVZUcJZ",
        "outputId": "40b4566a-a3e3-49cd-eb45-e35528943964"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nuevo estado: B\n",
            "Recompensa recibida: -1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearning:\n",
        "    def __init__(self, estados, acciones, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
        "        self.estados = estados\n",
        "        self.acciones = acciones\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.q_table = {}\n",
        "\n",
        "    def actualizar_q_table(self, estado_actual, accion, recompensa, nuevo_estado):\n",
        "        if (estado_actual) not in self.q_table:\n",
        "            self.q_table[(estado_actual)] = {a:0 for a in self.acciones}\n",
        "        if (nuevo_estado) not in self.q_table:\n",
        "            self.q_table[(nuevo_estado)] = {a:0 for a in self.acciones}\n",
        "\n",
        "        q_actual = self.q_table[(estado_actual)][accion]\n",
        "        max_q_nuevo_estado = max(self.q_table[(nuevo_estado)].values())\n",
        "        nuevo_q_valor = q_actual + self.alpha * (recompensa + self.gamma * max_q_nuevo_estado - q_actual)\n",
        "        self.q_table[(estado_actual)][accion] = nuevo_q_valor\n",
        "\n",
        "# Uso del algoritmo Q-learning\n",
        "estados = ['A', 'B', 'C']\n",
        "acciones = ['izquierda', 'derecha']\n",
        "q_learning = QLearning(estados, acciones)\n",
        "\n",
        "# Simulación de una época de entrenamiento\n",
        "estado_actual = 'A'\n",
        "accion = 'izquierda'\n",
        "nuevo_estado = 'B'\n",
        "recompensa = 10\n",
        "q_learning.actualizar_q_table(estado_actual, accion, recompensa, nuevo_estado)\n",
        "\n",
        "# Visualización de la tabla Q\n",
        "print(\"Tabla Q actualizada:\")\n",
        "print(q_learning.q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaygjqq8VHb2",
        "outputId": "198e0942-8bc7-48e5-ce7a-edf10c0af4b0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tabla Q actualizada:\n",
            "{'A': {'izquierda': 1.0, 'derecha': 0}, 'B': {'izquierda': 0, 'derecha': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6w_ikWu8VUWD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}