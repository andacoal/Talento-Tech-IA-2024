{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Actividad\n",
        "\n",
        "\n",
        "---\n",
        "Por: Ángela Córdoba\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gGN1lsJwiVRe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "X848PyEIiPhe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio 1: Implementación de Q-Learning en un entorno de gridworld simple"
      ],
      "metadata": {
        "id": "1M7Ew9GTigGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definición del gridworld\n",
        "gridworld = np.array([\n",
        "    [-1, -1, -1, 1],\n",
        "    [-1, -1, -1, -1],\n",
        "    [-1, -1, -1, -1],\n",
        "    [-1, -1, -1, -1],\n",
        "])\n",
        "\n",
        "# Definición de las acciones posibles: arriba, abajo, izquierda, derecha\n",
        "acciones = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
        "num_acciones = len(acciones)\n",
        "\n",
        "# Implementación de Q-Learning\n",
        "# Se agrega la dimensión de las acciones al inicializar Q\n",
        "Q = np.zeros((gridworld.shape[0], gridworld.shape[1], num_acciones))\n",
        "\n",
        "# Hiperparámetros del algoritmo\n",
        "gamma = 0.8\n",
        "alpha = 0.1\n",
        "num_episodios = 1000\n",
        "\n",
        "for _ in range(num_episodios):\n",
        "    estado = (0, 0)\n",
        "    while estado != (0, 3):\n",
        "        # Selecciona una acción basada en la política epsilon-greedy, aquí elegimos aleatoriamente\n",
        "        accion = np.random.choice(range(len(acciones)))\n",
        "        nueva_fila = estado[0] + acciones[accion][0] # Corrección: Se usa \"accion\" para indexar acciones\n",
        "        nueva_col = estado[1] + acciones[accion][1] # Corrección: Se usa \"accion\" para indexar acciones\n",
        "\n",
        "        # Verifica si el nuevo estado es válido\n",
        "        if 0 <= nueva_fila < gridworld.shape[0] and 0 <= nueva_col < gridworld.shape[1]:\n",
        "            recompensa = gridworld[nueva_fila, nueva_col]\n",
        "            # Calcular el nuevo valor Q\n",
        "            nuevo_valor = recompensa + gamma * np.max(Q[nueva_fila, nueva_col])\n",
        "            # Actualiza el valor Q para el estado y acción actuales\n",
        "            Q[estado[0], estado[1], accion] = (1 - alpha) * Q[estado[0], estado[1], accion] + alpha * nuevo_valor # Corrección: Indexación correcta de Q\n",
        "            # Actualiza el estado\n",
        "            estado = (nueva_fila, nueva_col)\n",
        "\n",
        "print(\"Q-values después del entrenamiento:\")\n",
        "print(Q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIXQPLoyieB6",
        "outputId": "e6df84da-fc2c-4ace-a8ea-589788865735"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-values después del entrenamiento:\n",
            "[[[ 0.   -1.    0.   -1.  ]\n",
            "  [ 0.   -1.8  -1.   -0.2 ]\n",
            "  [ 0.   -1.16 -1.    1.  ]\n",
            "  [ 0.    0.    0.    0.  ]]\n",
            "\n",
            " [[-1.   -1.    0.   -1.8 ]\n",
            "  [-1.   -1.8  -1.   -1.16]\n",
            "  [-0.2  -1.8  -1.8  -0.2 ]\n",
            "  [ 1.   -1.   -1.16  0.  ]]\n",
            "\n",
            " [[-1.   -1.    0.   -1.8 ]\n",
            "  [-1.8  -1.   -1.   -1.8 ]\n",
            "  [-1.16 -1.   -1.8  -1.  ]\n",
            "  [-0.2  -1.   -1.8   0.  ]]\n",
            "\n",
            " [[-1.    0.    0.   -1.  ]\n",
            "  [-1.8   0.   -1.   -1.  ]\n",
            "  [-1.8   0.   -1.   -1.  ]\n",
            "  [-1.    0.   -1.    0.  ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio 2: Aplicación del Aprendizaje por Refuerzo en juegos"
      ],
      "metadata": {
        "id": "8kb2_R9Ojif-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo: Implementación de Q-Learning para un juego simple\n",
        "#Definición de las recompensas del juego (datos ficticios)\n",
        "recompensa = {\n",
        "    'ganar': 1,\n",
        "    'perder': -1,\n",
        "    'empatar': 0,\n",
        "}\n",
        "\n",
        "#Implementación de Q-Learning para el juego\n",
        "Q={}\n",
        "\n",
        "def q_learning_juego(estado_actual, accion, nuevo_estado, resultado):\n",
        "    if (estado_actual, accion) not in Q:\n",
        "        Q[(estado_actual, accion)] = np.zeros(len(acciones)) # Inicializa Q[(estado, accion)] si no existe\n",
        "    if nuevo_estado not in Q:\n",
        "       Q[nuevo_estado] = np.zeros(len(acciones))\n",
        "    nuevo_valor = recompensa[resultado] + gamma * np.max(Q[nuevo_estado])\n",
        "    Q[(estado_actual, accion)] = (1-alpha) * Q[(estado_actual, accion)] + alpha * nuevo_valor # Accede a Q usando (estado, accion) como clave\n",
        "\n",
        "# Ejemplo de variables de entrada (deberían ser determinadas por la lógica del juego)\n",
        "estado_actual = 'estado_inicial'\n",
        "accion = 'abajo'\n",
        "nuevo_estado = 'estado_siguiente'\n",
        "resultado = 'ganar'  # Este valor podría ser 'ganar', 'perder', o 'empatar'\n",
        "\n",
        "# Llamada a la función Q-Learning con las variables de ejemplo\n",
        "q_learning_juego(estado_actual, accion, nuevo_estado, resultado)\n",
        "\n",
        "# Q después del entrenamiento\n",
        "print(\"Tabla Q-values después del entrenamiento:\")\n",
        "print(Q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tm_Lng7tjV4q",
        "outputId": "0eee2940-1de7-4213-ee3a-212c1958aa6c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tabla Q-values después del entrenamiento:\n",
            "{('estado_inicial', 'abajo'): array([0.1, 0.1, 0.1, 0.1]), 'estado_siguiente': array([0., 0., 0., 0.])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio 3: Aplicación del Aprendizaje por Refuerzo en robótica"
      ],
      "metadata": {
        "id": "_UBglZXhkWx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Definición del entorno de navegación (una matriz con valores ficticios)\n",
        "# 0 representa un espacio libre y -1 representa una barrera (obstáculo)\n",
        "entorno = np.array([\n",
        "    [0, 0, 0, 0, 0],\n",
        "    [0, -1, -1, -1, 0],\n",
        "    [0, 0, -1, 0, 0],\n",
        "    [0, -1, -1, -1, 0],\n",
        "    [0, 0, 0, 0, 0]\n",
        "])\n",
        "\n",
        "# Definición de las posibles acciones (movimientos) con sus respectivos cambios de coordenadas\n",
        "# Cada acción corresponde a: (arriba, abajo, izquierda, derecha)\n",
        "acciones = [(0, -1), (0, 1), (-1, 0), (1, 0)]\n",
        "\n",
        "# Inicialización de la tabla Q con ceros\n",
        "# Dimensiones: (filas del entorno, columnas del entorno, número de acciones posibles)\n",
        "Q = np.zeros((entorno.shape[0], entorno.shape[1], len(acciones)))\n",
        "\n",
        "# Parámetros de Q-Learning\n",
        "gamma = 0.9  # Factor de descuento: cuánto valoramos las recompensas futuras\n",
        "alpha = 0.1  # Tasa de aprendizaje: cuánto ajustamos los valores Q\n",
        "num_episodes = 1000  # Número de episodios de entrenamiento\n",
        "\n",
        "# Definición del estado objetivo o terminal (coordenadas del objetivo en la matriz)\n",
        "estado_objetivo = (4, 4)\n",
        "\n",
        "# Bucle principal de entrenamiento por episodios\n",
        "for _ in range(num_episodes):\n",
        "    estado = (0, 0)  # Estado inicial (posición de inicio)\n",
        "\n",
        "    # Continuar hasta que se alcance el estado objetivo\n",
        "    while estado != estado_objetivo:\n",
        "        # Selecciona una acción al azar entre las posibles (exploración)\n",
        "        accion = np.random.choice(range(len(acciones)))\n",
        "        # Calcula las nuevas coordenadas después de realizar la acción\n",
        "        nueva_fila = estado[0] + acciones[accion][0]\n",
        "        nueva_col = estado[1] + acciones[accion][1]\n",
        "\n",
        "        # Verifica si la nueva posición está dentro de los límites del entorno\n",
        "        if 0 <= nueva_fila < entorno.shape[0] and 0 <= nueva_col < entorno.shape[1]:\n",
        "            # Recompensa es el valor del entorno en la nueva posición\n",
        "            recompensa = entorno[nueva_fila, nueva_col]\n",
        "\n",
        "            # Calcula el nuevo valor de Q usando la fórmula de actualización de Q-Learning\n",
        "            nuevo_valor = recompensa + gamma * np.max(Q[nueva_fila, nueva_col, :])\n",
        "\n",
        "            # Actualiza el valor Q para el estado y acción actuales\n",
        "            Q[estado[0], estado[1], accion] = (1 - alpha) * Q[estado[0], estado[1], accion] + alpha * nuevo_valor\n",
        "\n",
        "            # Actualiza el estado actual al nuevo estado\n",
        "            estado = (nueva_fila, nueva_col)\n",
        "\n",
        "# Imprime la tabla de valores Q después del entrenamiento\n",
        "print(\"Valores Q después del entrenamiento:\")\n",
        "print(Q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0ysIlfEucYN",
        "outputId": "ee4cf1e5-325a-4e53-ff32-739a33f3df6e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valores Q después del entrenamiento:\n",
            "[[[ 0.  0.  0.  0.]\n",
            "  [ 0.  0.  0. -1.]\n",
            "  [ 0.  0.  0. -1.]\n",
            "  [ 0.  0.  0. -1.]\n",
            "  [ 0.  0.  0.  0.]]\n",
            "\n",
            " [[ 0. -1.  0.  0.]\n",
            "  [ 0. -1.  0.  0.]\n",
            "  [-1. -1.  0. -1.]\n",
            "  [-1.  0.  0.  0.]\n",
            "  [-1.  0.  0.  0.]]\n",
            "\n",
            " [[ 0.  0.  0.  0.]\n",
            "  [ 0. -1. -1. -1.]\n",
            "  [ 0.  0. -1. -1.]\n",
            "  [-1.  0. -1. -1.]\n",
            "  [ 0.  0.  0.  0.]]\n",
            "\n",
            " [[ 0. -1.  0.  0.]\n",
            "  [ 0. -1.  0.  0.]\n",
            "  [-1. -1. -1.  0.]\n",
            "  [-1.  0.  0.  0.]\n",
            "  [-1.  0.  0.  0.]]\n",
            "\n",
            " [[ 0.  0.  0.  0.]\n",
            "  [ 0.  0. -1.  0.]\n",
            "  [ 0.  0. -1.  0.]\n",
            "  [ 0.  0. -1.  0.]\n",
            "  [ 0.  0.  0.  0.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio 4: Aplicación del Aprendizaje por Refuerzo en gestión de recursos"
      ],
      "metadata": {
        "id": "RECscvPQk6SO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Definición de los estados (niveles de inventario), acciones (órdenes de reabastecimiento) y recompensar (costos, ganancias, etc.)\n",
        "\n",
        "estados = ['Bajo', 'Medio', 'Alto']\n",
        "acciones = ['Reabastecer', 'No reabastecer']\n",
        "recompensas = {\n",
        "    ('Bajo', 'Reabastecer'): 50,\n",
        "    ('Bajo', 'No reabastecer'): -10,\n",
        "    ('Medio', 'Reabastecer'): 30,\n",
        "    ('Medio', 'No reabastecer'): 0,\n",
        "    ('Alto', 'Reabastecer'): 10,\n",
        "    ('Alto', 'No reabastecer'): -20,\n",
        "}\n",
        "\n",
        "# Implementación de Q-Learning\n",
        "Q = {}\n",
        "\n",
        "gamma = 0.9 #Factor de descuento\n",
        "alpha = 0.1 #Tasa de aprendizaje\n",
        "num_episodes = 1000\n",
        "\n",
        "for _ in range(num_episodes):\n",
        "    estado_actual = np.random.choice(estados)\n",
        "    while True:\n",
        "        accion = np.random.choice(acciones)\n",
        "        recompensa = recompensas[(estado_actual, accion)]\n",
        "\n",
        "        if (estado_actual) not in Q:\n",
        "            Q[estado_actual] = {}\n",
        "        if accion not in Q[estado_actual]:\n",
        "            Q[estado_actual][accion] = 0\n",
        "\n",
        "        nuevo_estado = np.random.choice(estados)\n",
        "        max_nuevo_estado = max(Q[nuevo_estado].values()) if nuevo_estado in Q else 0\n",
        "        Q[estado_actual][accion] += alpha * (recompensa + gamma * max_nuevo_estado - Q[estado_actual][accion])\n",
        "        estado_actual = nuevo_estado\n",
        "        if recompensa == 50 or recompensa ==30 or recompensa == 10:\n",
        "           break\n",
        "\n",
        "print(\"Valores Q después del entrenamiento:\")\n",
        "print(Q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzJJ-Qw4k4FG",
        "outputId": "f1dad4e4-1204-455a-94c2-f0482b5b0acc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valores Q después del entrenamiento:\n",
            "{'Medio': {'No reabastecer': 257.2778356426104, 'Reabastecer': 284.03890879781153}, 'Bajo': {'Reabastecer': 309.23694478081853, 'No reabastecer': 251.26408542712733}, 'Alto': {'Reabastecer': 265.6234249209327, 'No reabastecer': 239.8940397309074}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1qwNwVmqvG8m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}