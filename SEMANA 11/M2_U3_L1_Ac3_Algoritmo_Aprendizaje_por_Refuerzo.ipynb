{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Actividad\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Por: Ángela Córdoba\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "kZhd0GjYT2S2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hB6nIM55TwuI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Genera un entorno de juego simple donde un agente debe aprender a navegar y recolectar objetos. Define estados, acciones y recompensas para el agente."
      ],
      "metadata": {
        "id": "6XPulGGiasht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejercicio 1: Introducción a los principales algoritmos de RL\n",
        "# Define el entorno del juego\n",
        "\n",
        "class Environment:\n",
        "    def __init__(self):\n",
        "        self.state_space = np.array([0, 1, 2, 3]) # Estados posibles\n",
        "        self.action_space = np.array([0, 1]) # Acciones posibles\n",
        "        self.rewards = {0: -1, 1: -1, 2: -1, 3: 10} # Recompensas por estado\n",
        "\n",
        "# Crea una instancia del entorno\n",
        "env = Environment()\n",
        "\n",
        "# Muestra información del entorno\n",
        "print(\"Estados posibles:\", env.state_space)\n",
        "print(\"Acciones posibles:\", env.action_space)\n",
        "print(\"Recompensas por estado:\", env.rewards)"
      ],
      "metadata": {
        "id": "oVADIiTHZQBe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2399ad1b-dbab-4027-eb25-7b1facf6bee3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estados posibles: [0 1 2 3]\n",
            "Acciones posibles: [0 1]\n",
            "Recompensas por estado: {0: -1, 1: -1, 2: -1, 3: 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementa el algoritmo Q-Learning para que\n",
        "un agente aprenda a navegar y recolectar\n",
        "objetos en el entorno definido. Muestra cómo se actualiza la función Q-valor."
      ],
      "metadata": {
        "id": "_4S3WS3WavwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejercicio 2: Q-Learning\n",
        "# Inicializa la tabla Q con valores arbitrarios\n",
        "Q = np.zeros((len(env.state_space), len(env.action_space)))\n",
        "\n",
        "# Define los parámetros del algortimo\n",
        "alpha = 0.1 #T asa de aprendizaje\n",
        "gamma = 0.9 # Factor de descuento\n",
        "\n",
        "# Entrena el agente utilizando Q-Learning\n",
        "for _ in range(1000):\n",
        "    state = np.random.choice(env.state_space)  # Estado inicial aleatorio\n",
        "    while state != 3:  # Hasta llegar al estado objetivo\n",
        "        action = np.random.choice(env.action_space)  # Selecciona una acción aleatoria\n",
        "        next_state = state + action\n",
        "        reward = env.rewards[next_state]\n",
        "        Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
        "        state = next_state\n",
        "\n",
        "# Muestra la función q-valor aprendida\n",
        "print(\"Función Q-valor aprendida:\")\n",
        "print(Q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9qLAXBgaHIO",
        "outputId": "5deed400-b29a-4b10-8f9f-42b046eb3c93"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Función Q-valor aprendida:\n",
            "[[ 4.58  6.2 ]\n",
            " [ 6.2   8.  ]\n",
            " [ 8.   10.  ]\n",
            " [ 0.    0.  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementa el algoritmo Sarsa para\n",
        "compararlo con Q-Learning en el mismo\n",
        "entorno. Muestra cómo se actualiza la\n",
        "función Q-valor y compara los resultados."
      ],
      "metadata": {
        "id": "serxhK5eazD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejercicio 3: Sarsa\n",
        "# Reinicializa la tabla Q con valores arbitrarios\n",
        "Q = np.zeros((len(env.state_space), len(env.action_space)))\n",
        "\n",
        "# Entrena el agente utilizando Sarsa\n",
        "for _ in range(1000):\n",
        "    state = np.random.choice(env.state_space) # Estado inicial\n",
        "    action = np.random.choice(env.action_space) # Selecciona una acción aleatoria\n",
        "    while state != 3: #Hasta llegar al estado objetivo\n",
        "        next_state = state + action\n",
        "        next_action = np.random.choice(env.action_space) # Selecciona una acción aleatoria\n",
        "        reward = env.rewards[next_state]\n",
        "        Q[state, action] = Q[state, action] + alpha * (reward + gamma *\n",
        "                                                       Q[next_state, next_action]\n",
        "                                                       - Q[state, action])\n",
        "        state = next_state\n",
        "        action = next_action\n",
        "\n",
        "# Muestra la función Q-valor aprendida con Sarsa\n",
        "print(\"Función Q-valor aprendida con Sarsa:\")\n",
        "print(Q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mq-W4-4FapDq",
        "outputId": "26e19eb2-9191-47a9-ce99-0f4e398d1389"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Función Q-valor aprendida con Sarsa:\n",
            "[[ 1.68870163  3.96783609]\n",
            " [ 4.17200751  6.28116411]\n",
            " [ 6.51747207 10.        ]\n",
            " [ 0.          0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementa la técnica de optimización basada en\n",
        "gradientes para aprender una política en el mismo entorno. Muestra cómo se actualizan los parámetros de la política utilizando el gradiente ascendente."
      ],
      "metadata": {
        "id": "63QsRXGNbOT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejercicio 4: Política de Gradiente de Montecarlo\n",
        "# Inicializa la política con probabilidades uniformes\n",
        "policy = np.ones((len(env.state_space), len(env.action_space))) / len(env.action_space)\n",
        "\n",
        "# Define la función de recompensa promedio\n",
        "def average_reward(Q):\n",
        "    return np.mean([Q[state, np.argmax(policy[state])] for state in env.state_space])\n",
        "\n",
        "# Entrena la política utilizando Gradiente de Montecarlo\n",
        "for _ in range(1000):\n",
        "    state = np.random.choice(env.state_space)  # Estado inicial aleatorio\n",
        "    while state != 3:  # Hasta el estado objetivo\n",
        "        action = np.random.choice(env.action_space, p=policy[state])  # Selecciona una acción basada en la política\n",
        "        next_state = state + action  # Estado siguiente (ajustar según el entorno real)\n",
        "        reward = env.rewards[next_state]  # Recompensa por el estado siguiente\n",
        "        gradient = np.zeros_like(policy[state])\n",
        "        gradient[action] = 1\n",
        "        alpha = 0.01\n",
        "        policy[state] += alpha * gradient * (reward - average_reward(Q))\n",
        "        policy[state] = np.exp(policy[state])  # Utiliza la función exponencial para asegurar valores positivos\n",
        "        policy[state] /= np.sum(policy[state])\n",
        "        state = next_state  # Actualiza el estado\n",
        "\n",
        "# Muestra la política aprendida\n",
        "print(\"Política aprendida con Gradiente de Montecarlo:\")\n",
        "print(policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXiOrlOXbMlR",
        "outputId": "0b4d3d5e-b350-4c8e-8a86-f80bf86fc22c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Política aprendida con Gradiente de Montecarlo:\n",
            "[[0.51891942 0.48108058]\n",
            " [0.50865296 0.49134704]\n",
            " [0.4724252  0.5275748 ]\n",
            " [0.5        0.5       ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y75mBhO2cAiF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}