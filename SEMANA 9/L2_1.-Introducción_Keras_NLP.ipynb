{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Ejemplo de Tokenización con Keras\n","Convertir secuencias de texto en secuencias de números enteros.\n","1. **Tokenización:**\n","Dividir el texto en tokens o palabras individuales. La tokenización también puede incluir la eliminación de signos de puntuación, caracteres especiales y conversiones a minúsculas para estandarizar el texto y facilitar su procesamiento.\n","2. **Indexación:**\n","El Tokenizer asigna un índice único a cada palabra en\n","el vocabulario, basado en la frecuencia de aparición de cada palabra en el corpus de texto.\n","Esta indexación es importante porque convierte las palabras en representaciones\n","numéricas.\n","3. **Vectorización:**\n","Las secuencias de palabras se\n","convierten en secuencias de números enteros, utilizando los índices asignados durante la\n","indexación. Este paso es esencial para alimentar los datos al modelo de red neuronal, ya\n","que las redes neuronales requieren entradas numéricas para realizar cálculos y aprender\n","patrones en los datos.\n","4. **Padding:**\n","El relleno implica agregar ceros al principio o al final de las secuencias para que todas tengan la misma longitud, lo que permite procesar lotes de\n","datos de manera eficiente. Por otro lado, el truncamiento limita la longitud de las secuencias para unificar su tamaño y mejorar la velocidad de procesamiento."],"metadata":{"id":"l4qIJiKIdr-F"}},{"cell_type":"markdown","source":["\n","\n","---\n","Por: Ángela Córdoba\n","\n","\n","---\n","\n","\n"],"metadata":{"id":"uFO3YhZ67r5t"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6LlUWOirdfIM"},"outputs":[],"source":["import keras # Se importa este módulo, pues contiene la clase Tokenizer\n","\n","# Se define una lista que contiene tres cadenas de texto\n","frases = [\n","    'Hola mundo',\n","    'Hola a todos',\n","    'Hola a todo el mundo'\n","]"]},{"cell_type":"markdown","source":["Se crea una instancia de la clase Tokenizer con el parámetro \"num_words\"\n","establecido en 10. Esto significa que solo se considerarán las 10 palabras más\n","comunes en el conjunto de texto.\n","La instancia del Tokenizer se ajusta a las frases proporcionadas mediante\n","el método \"t_on_texts\", lo que genera un diccionario de tokens basado en\n","las palabras presentes en las frases."],"metadata":{"id":"S6FZEAEFenIF"}},{"cell_type":"code","source":["# Cargar tensorflow, pues la opción .text no está disponible en Keras\n","!pip install tensorflow\n","import tensorflow as tf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cp-_6oeWfUPe","outputId":"00ccab84-961d-4d3c-ac9a-3fc33927d284"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.31.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n","Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n","Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.7.1)\n","Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n","Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.6)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"]}]},{"cell_type":"code","source":["# Generar el diccionario de tokens\n","tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10)\n","tokenizer.fit_on_texts(frases)\n","\n","# Obtener el diccionario de tokens\n","word_index = tokenizer.word_index\n","print('word_index =',word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cGzzMqwDeDe1","outputId":"8681c7e9-243a-4182-a98a-619bb261508b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["word_index = {'hola': 1, 'mundo': 2, 'a': 3, 'todos': 4, 'todo': 5, 'el': 6}\n"]}]},{"cell_type":"markdown","source":["Se llama al método \"texts_to_sequences\" de Tokenizer para convertir las frases en secuencias de\n","tokens. Esto reemplaza cada palabra en las frases con su token correspondiente según el\n","diccionario generado previamente."],"metadata":{"id":"2MWq52IMfjaW"}},{"cell_type":"code","source":["# Generar secuencias tokenizadas\n","secuencias = tokenizer.texts_to_sequences(frases)\n","print('secuencias =',secuencias)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oZdB7eDGfQDF","outputId":"1ff3e2e4-02ae-4350-fdfc-7e9ed11c96e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["secuencias = [[1, 2], [1, 3, 4], [1, 3, 5, 6, 2]]\n"]}]},{"cell_type":"markdown","source":["Se llama al método \"pad_sequences\" de la clase preprocessing.sequence para rellenar las\n","secuencias de tokens a una longitud uniforme. Por defecto, este método rellena con ceros al\n","principio de cada secuencia hasta alcanzar la longitud de la secuencia más larga."],"metadata":{"id":"j6cspgQDft9F"}},{"cell_type":"code","source":["# Rellenar las secuencias a una longitud uniforme\n","relleno = tf.keras.preprocessing.sequence.pad_sequences(secuencias)\n","print('relleno =\\n',relleno)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HB4cJPsRfqo-","outputId":"463785a5-a566-45d5-e1e1-e89fb76c2dd5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["relleno =\n"," [[0 0 0 1 2]\n"," [0 0 1 3 4]\n"," [1 3 5 6 2]]\n"]}]},{"cell_type":"code","source":["# Cambios en el código\n","frases = [\n","    'Hola mundo',\n","    'Hola a todos',\n","    'Hola a todo el mundo',\n","    'Buen dia, como estas hoy'\n","]\n","\n","# Generar el diccionario de tokens\n","tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10)\n","tokenizer.fit_on_texts(frases)\n","\n","# Obtener el diccionario de tokens\n","word_index = tokenizer.word_index\n","print('\\nword_index =',word_index)\n","\n","# Generar secuencias tokenizadas\n","secuencias = tokenizer.texts_to_sequences(frases)\n","print('secuencias =',secuencias)\n","\n","# Rellenar las secuencias a una longitud uniforme\n","relleno = tf.keras.preprocessing.sequence.pad_sequences(secuencias)\n","print('relleno =\\n',relleno)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XbQOAzoqf2qd","outputId":"62a56df3-28cf-4391-b17b-7a5f881dd7b3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","word_index = {'hola': 1, 'mundo': 2, 'a': 3, 'todos': 4, 'todo': 5, 'el': 6, 'buen': 7, 'dia': 8, 'como': 9, 'estas': 10, 'hoy': 11}\n","secuencias = [[1, 2], [1, 3, 4], [1, 3, 5, 6, 2], [7, 8, 9]]\n","relleno =\n"," [[0 0 0 1 2]\n"," [0 0 1 3 4]\n"," [1 3 5 6 2]\n"," [0 0 7 8 9]]\n"]}]},{"cell_type":"markdown","source":["* ¿Qué pasa si el número de palabra\n","en las frase es mayor al parámetro\n","(num_words)?"],"metadata":{"id":"w45wlXQUhC9-"}},{"cell_type":"code","source":["frases = [\n","    'Hola mundo',\n","    'Hola a todos',\n","    'Hola a todo el mundo',\n","    'Buen dia, como estas hoy. Espero estes muy pero muy bien' # Número de palabras aumenta a 11\n","]\n","\n","# Generar el diccionario de tokens\n","tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10)\n","tokenizer.fit_on_texts(frases)\n","\n","# Obtener el diccionario de tokens\n","word_index = tokenizer.word_index\n","print('\\nword_index =',word_index)\n","\n","# Generar secuencias tokenizadas\n","secuencias = tokenizer.texts_to_sequences(frases)\n","print('secuencias =',secuencias)\n","\n","# Rellenar las secuencias a una longitud uniforme\n","relleno = tf.keras.preprocessing.sequence.pad_sequences(secuencias)\n","print('relleno =\\n',relleno)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a57r0ZtEgaTJ","outputId":"32c1b158-07fd-426a-e271-b8af5a9c1f8e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","word_index = {'hola': 1, 'mundo': 2, 'a': 3, 'muy': 4, 'todos': 5, 'todo': 6, 'el': 7, 'buen': 8, 'dia': 9, 'como': 10, 'estas': 11, 'hoy': 12, 'espero': 13, 'estes': 14, 'pero': 15, 'bien': 16}\n","secuencias = [[1, 2], [1, 3, 5], [1, 3, 6, 7, 2], [8, 9, 4, 4]]\n","relleno =\n"," [[0 0 0 1 2]\n"," [0 0 1 3 5]\n"," [1 3 6 7 2]\n"," [0 8 9 4 4]]\n"]}]},{"cell_type":"markdown","source":["No sucede nada extraordinario. El código sigue funcionando normalmente y le asigna un token a cada palabra que sea superior al parámetro."],"metadata":{"id":"mKKhkTIRhjSY"}},{"cell_type":"markdown","source":["* Incluir el parámetro OOV"],"metadata":{"id":"D1wDLLK0htMg"}},{"cell_type":"code","source":["frases = [\n","    'Hola mundo',\n","    'Hola a todos',\n","    'Hola a todo el mundo',\n","    'Buen dia, como estas hoy'\n","]\n","\n","# Generar el diccionario de tokens\n","tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10,\n","                                                  oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(frases)\n","\n","# Obtener el diccionario de tokens\n","word_index = tokenizer.word_index\n","print('\\nword_index =',word_index)\n","\n","# Generar secuencias tokenizadas\n","secuencias = tokenizer.texts_to_sequences(frases)\n","print('secuencias =',secuencias)\n","\n","# Rellenar las secuencias a una longitud uniforme\n","relleno = tf.keras.preprocessing.sequence.pad_sequences(secuencias)\n","print('relleno =\\n',relleno)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BezCvtiChYsV","outputId":"cce48da3-2f04-4090-c7ac-4d339b014445"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","word_index = {'<OOV>': 1, 'hola': 2, 'mundo': 3, 'a': 4, 'todos': 5, 'todo': 6, 'el': 7, 'buen': 8, 'dia': 9, 'como': 10, 'estas': 11, 'hoy': 12}\n","secuencias = [[2, 3], [2, 4, 5], [2, 4, 6, 7, 3], [8, 9, 1, 1, 1]]\n","relleno =\n"," [[0 0 0 2 3]\n"," [0 0 2 4 5]\n"," [2 4 6 7 3]\n"," [8 9 1 1 1]]\n"]}]},{"cell_type":"markdown","source":["Los parámetros padding y truncating de la función pad_sequences en Keras se utilizan para controlar el relleno y el truncamiento de las secuencias durante el\n","preprocesamiento de datos."],"metadata":{"id":"zaqU4TbJl9b_"}},{"cell_type":"code","source":["frases = [\n","    'Hola mundo',\n","    'Hola a todos',\n","    'Hola a todo el mundo',\n","    'Buen dia, como estas hoy'\n","]\n","\n","# Generar el diccionario de tokens\n","tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10,\n","                                                  oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(frases)\n","\n","# Obtener el diccionario de tokens\n","word_index = tokenizer.word_index\n","print('\\nword_index =',word_index)\n","\n","# Generar secuencias tokenizadas\n","secuencias = tokenizer.texts_to_sequences(frases)\n","print('secuencias =',secuencias)\n","\n","# Rellenar las secuencias a una longitud uniforme\n","relleno = tf.keras.preprocessing.sequence.pad_sequences(secuencias,\n","                                                        padding='post',\n","                                                        truncating='post')\n","print('relleno =\\n',relleno)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tyaZGGTelsjR","outputId":"fd5aa202-1fa7-4e53-f21e-345f913ae099"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","word_index = {'<OOV>': 1, 'hola': 2, 'mundo': 3, 'a': 4, 'todos': 5, 'todo': 6, 'el': 7, 'buen': 8, 'dia': 9, 'como': 10, 'estas': 11, 'hoy': 12}\n","secuencias = [[2, 3], [2, 4, 5], [2, 4, 6, 7, 3], [8, 9, 1, 1, 1]]\n","relleno =\n"," [[2 3 0 0 0]\n"," [2 4 5 0 0]\n"," [2 4 6 7 3]\n"," [8 9 1 1 1]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"eQsMg38Embqu"},"execution_count":null,"outputs":[]}]}